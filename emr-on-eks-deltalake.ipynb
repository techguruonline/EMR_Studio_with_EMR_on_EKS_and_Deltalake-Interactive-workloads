{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8572b039-b1c1-4efd-ac94-4cef7c4f4b83",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b04026-659d-4daf-9f05-c83b81851a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T09:03:18.683136Z",
     "iopub.status.busy": "2022-12-07T09:03:18.682652Z",
     "iopub.status.idle": "2022-12-07T09:03:18.687935Z",
     "shell.execute_reply": "2022-12-07T09:03:18.687054Z",
     "shell.execute_reply.started": "2022-12-07T09:03:18.683046Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Delta Lake is an open-source storage framework that enables building a Lakehouse architecture. It provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. It runs on top of your existing data lakes and is compatible with processing engines like Apache Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python. Specifically, it provides the following features:\n",
    "- __ACID guarantees__ <br>\n",
    "     Delta Lake ensures that all data changes written to storage are committed for durability and made visible to readers atomically. In other words, no more partial or corrupted files! We will discuss more on the acid guarantees as part of the transaction log later in this chapter.\n",
    "\n",
    "- __Scalable data and metadata handling:__<br>\n",
    "    Since Delta Lake is built on data lakes, all reads and writes using Spark or other distributed processing engines are inherently scalable to petabyte-scale. However, unlike most other storage formats and query engines, Delta\n",
    "    Lake leverages Spark to scale out all the metadata processing, thus efficiently handling metadata of billions of files for petabyte-scale tables. We will discuss more on the transaction log later in this chapter.\n",
    "\n",
    "- __Audit History and Time travel__<br>\n",
    "    The Delta Lake transaction log records details about every change made to data providing a full audit trail of the changes. These data snapshots enable developers to access and revert to earlier versions of data for audits, \n",
    "    rollbacks, or to reproduce experiments. We will dive further into this topic in Chapter 3: Time Travel with Delta.\n",
    "\n",
    "- __Schema enforcement and schema evolution__<br>\n",
    "    Delta Lake automatically prevents the insertion of data with an incorrect schema, i.e. not matching the table schema. And when needed, it allows the table schema to be explicitly and safely evolved to accommodate ever-change \n",
    "    data. We will dive further into this topic in Chapter 4 focusing on schema enforcement and evolution.\n",
    "\n",
    "- __Support for deletes updates, and merge__<br>\n",
    "    Most distributed processing frameworks do not support atomic data modification operations on data lakes. Delta Lake supports merge, update, and delete operations to enable complex use cases including but not limited to change-\n",
    "    datacapture (CDC), slowly-changing-dimension (SCD) operations, and streaming upserts. We will dive further into this topic in Chapter 5: Data modifications in Delta.\n",
    "\n",
    "- __Streaming and batch unification__<br>\n",
    "    A Delta Lake table has the ability to work both in batch and as a streaming source and sink. The ability to work across a wide variety of latencies ranging from streaming data ingest to batch historic backfill to interactive \n",
    "    queries all just work out of the box. We will dive further into this topic in Chapter 6: Streaming Applications with Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181efa7-b025-4189-ab4d-75d9a3a566c6",
   "metadata": {},
   "source": [
    "### For additional details, please refer to Apache Iceberg documentation:<br>\n",
    "- __[Deltalake Documentation](https://docs.delta.io/latest/index.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a5cbf-aa1b-45be-8697-728e0f8a0d77",
   "metadata": {},
   "source": [
    "## Pre-Requisite:\n",
    "\n",
    "For executing the code in this notebook you will need the below:\n",
    "- A AWS account <br>\n",
    "\n",
    "Below services should be created and configured\n",
    "- EMR Studio\n",
    "- EMR Studio Workspace\n",
    "- EMR on EKS Virtual Cluster\n",
    "- EKS Cluster (EC2 based)\n",
    "- Managed Endpoint\n",
    "- IAM Policy\n",
    "- Application Load Balancer\n",
    "- VPC and Subnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af00407-ebf5-482b-923e-734e90581e60",
   "metadata": {},
   "source": [
    "### Note: \n",
    "##### As of __EMR relase 6.9.0__ for EMR on EKS, EMR Studio notebooks support cell magic for configuring Spark parameters including passing the Deltalake Jar files. At this point just __%%configure__ is supported for cell magic.\n",
    "##### For more information on release notes, please refer to __[Release notes](https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks-6.9.0.html)__ <br>\n",
    "\n",
    "#### With cell magic support to override Spark parameters within the notebook itself, we now have multiple ways to configure the parameters\n",
    " - Defining the parameters within the Managed Endpoint which have shown how to configure for both __[Iceberg](https://github.com/techguruonline/EMR_Studio_with_EMR_on_EKS_and_Iceberg-Interactive-workloads)__ and __[Hudi](https://github.com/techguruonline/EMR_Studio_with_EMR_on_EKS_and_HUDI-Interactive-workloads)__\n",
    " - Defining the parameters within the EMR Studio notebook which can be used either to override the parameters defined in the Managed endpoint or just define it here instead of the Managed Endpoint. Have shown this in this notebook for Deltalake\n",
    " \n",
    " \n",
    "### Current Setup used for this notebook\n",
    "- EMR version: __emr-6.9.0-latest__\n",
    "- EKS version: __1.21__\n",
    "- Instance Type for EKS cluster: __m5.xlarge__\n",
    "- No of Instances: __3__\n",
    "- Deltalake Version: __Delta 2.1.0__\n",
    "- Spark Version: __3.3.0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "649c8430-176e-4311-aeda-8a1275933f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:22:01.165649Z",
     "iopub.status.busy": "2022-12-07T10:22:01.165121Z",
     "iopub.status.idle": "2022-12-07T10:22:19.137158Z",
     "shell.execute_reply": "2022-12-07T10:22:19.137024Z",
     "shell.execute_reply.started": "2022-12-07 10:22:01.167977+00:00"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2022-12-07 10:22:01,167.167 configure_magic] Magic cell payload received: {\"driverMemory\": \"1G\", \"driverCores\": 1, \"executorMemory\": \"1G\", \"executorCores\": 1, \"conf\": {\"spark.dynamicAllocation.maxExecutors\": 10, \"spark.dynamicAllocation.minExecutors\": 1, \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\", \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\", \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\", \"spark.databricks.hive.metastore.glueCatalog.enabled\": \"true\", \"spark.jars.packages\": \"io.delta:delta-core_2.12:2.1.0\", \"spark.databricks.delta.schema.autoMerge.enabled\": \"true\"}, \"proxyUser\": \"assumed-role_cf-emr-studio-1-StudioUserRole-1UMGXJ16SJMRN_emrstudio\"}\n",
      "\n",
      "[I 2022-12-07 10:22:01,168.168 configure_magic] Sending request to update kernel. Please wait while the kernel will be refreshed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel is successfully refreshed."
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "\n",
    "{\n",
    "  \"driverMemory\": \"1G\",\n",
    "  \"driverCores\" : 1,\n",
    "  \"executorMemory\" : \"1G\",\n",
    "  \"executorCores\": 1,\n",
    "  \"conf\": {\n",
    "     \"spark.dynamicAllocation.maxExecutors\" : 10,\n",
    "     \"spark.dynamicAllocation.minExecutors\": 1,\n",
    "     \"spark.sql.extensions\" : \"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "     \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "     \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "     \"spark.databricks.hive.metastore.glueCatalog.enabled\":\"true\",\n",
    "     \"spark.jars.packages\":\"io.delta:delta-core_2.12:2.1.0\",\n",
    "     \"spark.databricks.delta.schema.autoMerge.enabled\" : \"true\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb91edf5-2e84-4336-9f4f-aa6e748c9aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:22:22.171969Z",
     "iopub.status.busy": "2022-12-07T10:22:22.171395Z",
     "iopub.status.idle": "2022-12-07T10:22:22.243654Z",
     "shell.execute_reply": "2022-12-07T10:22:22.242413Z",
     "shell.execute_reply.started": "2022-12-07T10:22:22.171839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import few libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71eb7d2f-8fa8-4edb-b095-c90ed5a4e97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:22:25.563121Z",
     "iopub.status.busy": "2022-12-07T10:22:25.562094Z",
     "iopub.status.idle": "2022-12-07T10:22:32.391717Z",
     "shell.execute_reply": "2022-12-07T10:22:32.390267Z",
     "shell.execute_reply.started": "2022-12-07T10:22:25.563070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "InputData = [\n",
    "    (1,'Prasad Nadig', 25, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (2,'Ethereum', 80, 'NY', '2022-01-02', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (3,'Cosmos', 25, 'PA', '2022-01-03', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (4,'Solana', 55, 'MD', '2022-01-04', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (5,'Cardano', 15, 'TX', '2022-01-05', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (6,'Link', 45, 'NJ', '2022-01-06', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "inputDF = spark.createDataFrame(data=InputData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5cb786d-e6e1-4941-ad34-d47c2ac91e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:22:34.021256Z",
     "iopub.status.busy": "2022-12-07T10:22:34.020830Z",
     "iopub.status.idle": "2022-12-07T10:22:39.406902Z",
     "shell.execute_reply": "2022-12-07T10:22:39.405858Z",
     "shell.execute_reply.started": "2022-12-07T10:22:34.021224Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-12-07 10:22:25|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-07 10:22:25|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-07 10:22:25|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-07 10:22:25|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-07 10:22:25|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-07 10:22:25|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a7072b8-93b2-4b0f-8e9d-500beddde3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:22:43.301570Z",
     "iopub.status.busy": "2022-12-07T10:22:43.301085Z",
     "iopub.status.idle": "2022-12-07T10:23:12.998338Z",
     "shell.execute_reply": "2022-12-07T10:23:12.993111Z",
     "shell.execute_reply.started": "2022-12-07T10:22:43.301536Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write DataFrame as a Delta table\n",
    "inputDF.write.format(\"delta\") \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .option(\"overwriteSchema\", \"true\") \\\n",
    "       .partitionBy(\"create_date\") \\\n",
    "       .save(f\"s3://emr-studio-emr-on-eks/tmp/delta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc98f5b-9635-4739-b708-aa9df18daf5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:23:15.066907Z",
     "iopub.status.busy": "2022-12-07T10:23:15.066437Z",
     "iopub.status.idle": "2022-12-07T10:23:22.432630Z",
     "shell.execute_reply": "2022-12-07T10:23:22.431809Z",
     "shell.execute_reply.started": "2022-12-07T10:23:15.066863Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-12-07 10:22:25|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-07 10:22:25|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-07 10:22:25|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-07 10:22:25|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-07 10:22:25|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-07 10:22:25|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set path to the Delta table, read data from Delta table\n",
    "deltaPath = 's3://emr-studio-emr-on-eks/tmp/delta/'\n",
    "df_delta = spark.read.format(\"delta\").load(deltaPath)\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1fa86-a715-4929-8915-ccec67ac75d8",
   "metadata": {},
   "source": [
    "### UPSERT with Delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7088817e-7720-440f-a95f-1c8c15387e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:23:24.857445Z",
     "iopub.status.busy": "2022-12-07T10:23:24.856912Z",
     "iopub.status.idle": "2022-12-07T10:23:24.890828Z",
     "shell.execute_reply": "2022-12-07T10:23:24.889981Z",
     "shell.execute_reply.started": "2022-12-07T10:23:24.857415Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Update couple of existing records. Update cust_age to 35 for cust_id 1 and cust_loc from MD to CA for cust_id 4\n",
    "#Insert a new Record cust_id = 7\n",
    "UpdateData = [\n",
    "    (1,'Prasad Nadig', 35, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (4,'Solana', 55, 'MD', '2022-01-04', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (7,'Algorand', 55, 'NC', '2022-01-07', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "updateDF = spark.createDataFrame(data=UpdateData,schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9621f211-bd48-4690-bf5b-1b9567e6b063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:23:31.373708Z",
     "iopub.status.busy": "2022-12-07T10:23:31.373243Z",
     "iopub.status.idle": "2022-12-07T10:23:45.673217Z",
     "shell.execute_reply": "2022-12-07T10:23:45.672195Z",
     "shell.execute_reply.started": "2022-12-07T10:23:31.373676Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge both Source and Target data with conditions to either update the record if it exists based on the primary key (cust_id) else Insert if it doesn't exist in target.\n",
    "# NOTE: if you run this step, please ignore the next step, have shown different ways to perform UPSERT using Deltalake.\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.alias('targetData') \\\n",
    "    .merge(\n",
    "        updateDF.alias('updatedData'),\n",
    "        'targetData.cust_id = updatedData.cust_id') \\\n",
    "    .whenMatchedUpdate( set = \n",
    "        {\n",
    "            \"cust_id\": \"updatedData.cust_id\",\n",
    "            \"cust_name\": \"updatedData.cust_name\",\n",
    "            \"cust_age\": \"updatedData.cust_age\",\n",
    "            \"cust_loc\": \"updatedData.cust_loc\",\n",
    "            \"create_date\": \"updatedData.create_date\",\n",
    "            \"last_updated_time\": \"updatedData.last_updated_time\"\n",
    "        } \n",
    "                      ) \\\n",
    "    .whenNotMatchedInsert(values = \n",
    "        {\n",
    "            \"cust_id\": \"updatedData.cust_id\",\n",
    "            \"cust_name\": \"updatedData.cust_name\",\n",
    "            \"cust_age\": \"updatedData.cust_age\",\n",
    "            \"cust_loc\": \"updatedData.cust_loc\",\n",
    "            \"create_date\": \"updatedData.create_date\",\n",
    "            \"last_updated_time\": \"updatedData.last_updated_time\"\n",
    "        } \n",
    "                         ) \\\n",
    "    .execute()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa7c648-4799-4b15-9b91-bb95decdba09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:01:49.756323Z",
     "iopub.status.busy": "2022-12-06T17:01:49.755558Z",
     "iopub.status.idle": "2022-12-06T17:02:07.630075Z",
     "shell.execute_reply": "2022-12-06T17:02:07.622779Z",
     "shell.execute_reply.started": "2022-12-06T17:01:49.756271Z"
    }
   },
   "outputs": [],
   "source": [
    "# This step is optional, have shown different ways to UPSERT data in Deltalake. The above option provides you flexibility to update/insert only selective columns, this option will update all the columns and same for Insert as well.\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.alias('targetData') \\\n",
    "    .merge(\n",
    "        updateDF.alias('updatedData'),\n",
    "        'targetData.cust_id = updatedData.cust_id') \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffddb15d-4516-48fd-90c8-db882d2a44d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:23:49.290099Z",
     "iopub.status.busy": "2022-12-07T10:23:49.289680Z",
     "iopub.status.idle": "2022-12-07T10:23:50.772224Z",
     "shell.execute_reply": "2022-12-07T10:23:50.770878Z",
     "shell.execute_reply.started": "2022-12-07T10:23:49.290069Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      35|      NJ| 2022-01-01|2022-12-07 10:23:24|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-07 10:22:25|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-07 10:22:25|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-07 10:23:24|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-07 10:22:25|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-07 10:22:25|\n",
      "|      7|    Algorand|      55|      NC| 2022-01-07|2022-12-07 10:23:24|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta_updates = spark.read.format(\"delta\").load(deltaPath)\n",
    "df_delta_updates.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbc225-631a-4d05-a090-fade7a4ba876",
   "metadata": {},
   "source": [
    "### Delete records with Delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39915f4-be04-4092-86bd-1a0ed2a0c1a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:23:56.809297Z",
     "iopub.status.busy": "2022-12-07T10:23:56.808864Z",
     "iopub.status.idle": "2022-12-07T10:23:56.843571Z",
     "shell.execute_reply": "2022-12-07T10:23:56.842198Z",
     "shell.execute_reply.started": "2022-12-07T10:23:56.809265Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete an existing record. Records from source if matched with records in target will be deleted from target table.\n",
    "\n",
    "DeleteData = [\n",
    "    (6,'Link', 45, 'NJ', '2022-01-06', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "deleteDF = spark.createDataFrame(data=DeleteData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe1b12d-9b08-47c7-8832-ce4dced716c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:24:06.401462Z",
     "iopub.status.busy": "2022-12-07T10:24:06.400723Z",
     "iopub.status.idle": "2022-12-07T10:24:12.897841Z",
     "shell.execute_reply": "2022-12-07T10:24:12.896198Z",
     "shell.execute_reply.started": "2022-12-07T10:24:06.401409Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similar to Upsert, we use the .merge function from delta tables, if the record matches between source and target, the records will be deleted from the target table.\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.alias('targetData') \\\n",
    "    .merge(\n",
    "        deleteDF.alias('deletedData'),\n",
    "        'targetData.cust_id = deletedData.cust_id') \\\n",
    "    .whenMatchedDelete()\\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d5d3b17-0424-4702-ac86-351dc1d78eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:24:16.965396Z",
     "iopub.status.busy": "2022-12-07T10:24:16.964939Z",
     "iopub.status.idle": "2022-12-07T10:24:18.124712Z",
     "shell.execute_reply": "2022-12-07T10:24:18.123885Z",
     "shell.execute_reply.started": "2022-12-07T10:24:16.965364Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      35|      NJ| 2022-01-01|2022-12-07 10:23:24|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-07 10:22:25|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-07 10:22:25|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-07 10:23:24|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-07 10:22:25|\n",
      "|      7|    Algorand|      55|      NC| 2022-01-07|2022-12-07 10:23:24|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta_deletes = spark.read.format(\"delta\").load(deltaPath)\n",
    "df_delta_deletes.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e06fd-f648-4e1a-b65a-a876f678e926",
   "metadata": {},
   "source": [
    "### Time Travel with Deltalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2877bccb-a9dc-4677-bfee-954a3700b17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:24:22.869091Z",
     "iopub.status.busy": "2022-12-07T10:24:22.868667Z",
     "iopub.status.idle": "2022-12-07T10:24:24.223869Z",
     "shell.execute_reply": "2022-12-07T10:24:24.223097Z",
     "shell.execute_reply.started": "2022-12-07T10:24:22.869060Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2022-12-07 10:24:11|  null|    null|    MERGE|{predicate -> (ta...|null|    null|     null|          1|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|      1|2022-12-07 10:23:43|  null|    null|    MERGE|{predicate -> (ta...|null|    null|     null|          0|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|      0|2022-12-07 10:22:57|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List different versions\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a7d4b03-45bc-4ad6-8422-7d701fa91753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:24:27.976415Z",
     "iopub.status.busy": "2022-12-07T10:24:27.975896Z",
     "iopub.status.idle": "2022-12-07T10:24:31.510368Z",
     "shell.execute_reply": "2022-12-07T10:24:31.508978Z",
     "shell.execute_reply.started": "2022-12-07T10:24:27.976385Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-12-07 10:22:25|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-07 10:22:25|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-07 10:22:25|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-07 10:22:25|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-07 10:22:25|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-07 10:22:25|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's query the delta table to fetch the data before Upsert operation i.e, initial inserted data.\n",
    "beforeUpsert = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath)\n",
    "beforeUpsert.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "299eee86-2fd3-4c87-bb0a-7875b6b5ae00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:24:34.286164Z",
     "iopub.status.busy": "2022-12-07T10:24:34.285488Z",
     "iopub.status.idle": "2022-12-07T10:24:37.586195Z",
     "shell.execute_reply": "2022-12-07T10:24:37.584952Z",
     "shell.execute_reply.started": "2022-12-07T10:24:34.286115Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      35|      NJ| 2022-01-01|2022-12-07 10:23:24|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-07 10:22:25|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-07 10:22:25|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-07 10:23:24|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-07 10:22:25|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-07 10:22:25|\n",
      "|      7|    Algorand|      55|      NC| 2022-01-07|2022-12-07 10:23:24|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now let's query the delta table to fetch the data before the record was deleted\n",
    "beforeDelete = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(deltaPath)\n",
    "beforeDelete.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833da195-81bf-4e81-9a0b-d36811b58e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "spark_python_kubernetes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
