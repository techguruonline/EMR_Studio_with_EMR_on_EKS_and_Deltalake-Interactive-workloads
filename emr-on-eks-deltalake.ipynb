{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8572b039-b1c1-4efd-ac94-4cef7c4f4b83",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b04026-659d-4daf-9f05-c83b81851a93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T09:03:18.683136Z",
     "iopub.status.busy": "2022-12-07T09:03:18.682652Z",
     "iopub.status.idle": "2022-12-07T09:03:18.687935Z",
     "shell.execute_reply": "2022-12-07T09:03:18.687054Z",
     "shell.execute_reply.started": "2022-12-07T09:03:18.683046Z"
    }
   },
   "source": [
    "##### Delta Lake is an open-source storage framework that enables building a Lakehouse architecture. It provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. It runs on top of your existing data lakes and is compatible with processing engines like Apache Spark, PrestoDB, Flink, Trino, and Hive and APIs for Scala, Java, Rust, Ruby, and Python. Specifically, it provides the following features:\n",
    "> __ACID guarantees__ <br>\n",
    "    Delta Lake ensures that all data changes written to storage are committed for durability and made visible to readers atomically. In other words, no more partial or corrupted files! We will discuss more on the acid guarantees as part of the transaction log later in this chapter.\n",
    "\n",
    "> __Scalable data and metadata handling:__\n",
    "Since Delta Lake is built on data lakes, all reads and writes using Spark or other distributed processing engines are inherently scalable to petabyte-scale. However, unlike most other storage formats and query engines, Delta Lake leverages Spark to scale out all the metadata processing, thus efficiently handling metadata of billions of files for petabyte-scale tables. We will discuss more on the transaction log later in this chapter.\n",
    "\n",
    "> __Audit History and Time travel__\n",
    "The Delta Lake transaction log records details about every change made to data providing a full audit trail of the changes. These data snapshots enable developers to access and revert to earlier versions of data for audits, rollbacks, or to reproduce experiments. We will dive further into this topic in Chapter 3: Time Travel with Delta.\n",
    "\n",
    "> __Schema enforcement and schema evolution__\n",
    "Delta Lake automatically prevents the insertion of data with an incorrect schema, i.e. not matching the table schema. And when needed, it allows the table schema to be explicitly and safely evolved to accommodate ever-change data. We will dive further into this topic in Chapter 4 focusing on schema enforcement and evolution.\n",
    "\n",
    "> __Support for deletes updates, and merge__\n",
    "Most distributed processing frameworks do not support atomic data modification operations on data lakes. Delta Lake supports merge, update, and delete operations to enable complex use cases including but not limited to change-datacapture (CDC), slowly-changing-dimension (SCD) operations, and streaming upserts. We will dive further into this topic in Chapter 5: Data modifications in Delta.\n",
    "\n",
    "> __Streaming and batch unification__\n",
    "A Delta Lake table has the ability to work both in batch and as a streaming source and sink. The ability to work across a wide variety of latencies ranging from streaming data ingest to batch historic backfill to interactive queries all just work out of the box. We will dive further into this topic in Chapter 6: Streaming Applications with Delta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b181efa7-b025-4189-ab4d-75d9a3a566c6",
   "metadata": {},
   "source": [
    "### For additional details, please refer to Apache Iceberg documentation:<br>\n",
    "- __[Deltalake Documentation](https://docs.delta.io/latest/index.html)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5a5cbf-aa1b-45be-8697-728e0f8a0d77",
   "metadata": {},
   "source": [
    "## Pre-Requisite:\n",
    "\n",
    "For executing the code in this notebook you will need the below:\n",
    "- A AWS account <br>\n",
    "\n",
    "Below services should be created and configured\n",
    "- EMR Studio\n",
    "- EMR Studio Workspace\n",
    "- EMR on EKS Virtual Cluster\n",
    "- EKS Cluster (EC2 based)\n",
    "- Managed Endpoint\n",
    "- IAM Policy\n",
    "- Application Load Balancer\n",
    "- VPC and Subnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649c8430-176e-4311-aeda-8a1275933f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T16:59:00.452999Z",
     "iopub.status.busy": "2022-12-06T16:59:00.452334Z",
     "iopub.status.idle": "2022-12-06T16:59:19.955413Z",
     "shell.execute_reply": "2022-12-06T16:59:19.955300Z",
     "shell.execute_reply.started": "2022-12-06 16:59:00.467394+00:00"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2022-12-06 16:59:00,465.465 configure_magic] Magic cell payload received: {\"driverMemory\": \"1G\", \"driverCores\": 1, \"executorMemory\": \"1G\", \"executorCores\": 1, \"conf\": {\"spark.dynamicAllocation.maxExecutors\": 10, \"spark.dynamicAllocation.minExecutors\": 1, \"spark.sql.extensions\": \"io.delta.sql.DeltaSparkSessionExtension\", \"spark.serializer\": \"org.apache.spark.serializer.KryoSerializer\", \"spark.sql.catalog.spark_catalog\": \"org.apache.spark.sql.delta.catalog.DeltaCatalog\", \"spark.databricks.hive.metastore.glueCatalog.enabled\": \"true\", \"spark.jars.packages\": \"io.delta:delta-core_2.12:2.1.0\"}, \"proxyUser\": \"assumed-role_cf-emr-studio-1-StudioUserRole-1UMGXJ16SJMRN_emrstudio\"}\n",
      "\n",
      "[I 2022-12-06 16:59:00,467.467 configure_magic] Sending request to update kernel. Please wait while the kernel will be refreshed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The kernel is successfully refreshed."
     ]
    }
   ],
   "source": [
    "%%configure -f\n",
    "\n",
    "{\n",
    "  \"driverMemory\": \"1G\",\n",
    "  \"driverCores\" : 1,\n",
    "  \"executorMemory\" : \"1G\",\n",
    "  \"executorCores\": 1,\n",
    "  \"conf\": {\n",
    "     \"spark.dynamicAllocation.maxExecutors\" : 10,\n",
    "     \"spark.dynamicAllocation.minExecutors\": 1,\n",
    "     \"spark.sql.extensions\" : \"io.delta.sql.DeltaSparkSessionExtension\",\n",
    "     \"spark.serializer\":\"org.apache.spark.serializer.KryoSerializer\",\n",
    "     \"spark.sql.catalog.spark_catalog\":\"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "     \"spark.databricks.hive.metastore.glueCatalog.enabled\":\"true\",\n",
    "     \"spark.jars.packages\":\"io.delta:delta-core_2.12:2.1.0\",\n",
    "     \"spark.databricks.delta.schema.autoMerge.enabled\" : \"true\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb91edf5-2e84-4336-9f4f-aa6e748c9aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T16:59:51.340948Z",
     "iopub.status.busy": "2022-12-06T16:59:51.340073Z",
     "iopub.status.idle": "2022-12-06T16:59:51.382732Z",
     "shell.execute_reply": "2022-12-06T16:59:51.381408Z",
     "shell.execute_reply.started": "2022-12-06T16:59:51.340783Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from delta.tables import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71eb7d2f-8fa8-4edb-b095-c90ed5a4e97d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T16:59:54.490288Z",
     "iopub.status.busy": "2022-12-06T16:59:54.489476Z",
     "iopub.status.idle": "2022-12-06T16:59:59.036856Z",
     "shell.execute_reply": "2022-12-06T16:59:59.035501Z",
     "shell.execute_reply.started": "2022-12-06T16:59:54.490233Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "InputData = [\n",
    "    (1,'Prasad Nadig', 25, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (2,'Ethereum', 80, 'NY', '2022-01-02', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (3,'Cosmos', 25, 'PA', '2022-01-03', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (4,'Solana', 55, 'MD', '2022-01-04', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (5,'Carnado', 15, 'TX', '2022-01-05', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (6,'Link', 45, 'NJ', '2022-01-06', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "inputDF = spark.createDataFrame(data=InputData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb786d-e6e1-4941-ad34-d47c2ac91e0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a7072b8-93b2-4b0f-8e9d-500beddde3d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:00:01.088484Z",
     "iopub.status.busy": "2022-12-06T17:00:01.087729Z",
     "iopub.status.idle": "2022-12-06T17:00:36.119104Z",
     "shell.execute_reply": "2022-12-06T17:00:36.117826Z",
     "shell.execute_reply.started": "2022-12-06T17:00:01.088431Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write a DataFrame as a Delta dataset\n",
    "inputDF.write.format(\"delta\") \\\n",
    "       .mode(\"overwrite\") \\\n",
    "       .option(\"overwriteSchema\", \"true\") \\\n",
    "       .partitionBy(\"create_date\") \\\n",
    "       .save(f\"s3://emr-studio-emr-on-eks/tmp/delta/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc98f5b-9635-4739-b708-aa9df18daf5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:00:41.896096Z",
     "iopub.status.busy": "2022-12-06T17:00:41.895329Z",
     "iopub.status.idle": "2022-12-06T17:00:41.902342Z",
     "shell.execute_reply": "2022-12-06T17:00:41.900988Z",
     "shell.execute_reply.started": "2022-12-06T17:00:41.896043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "deltaPath = 's3://emr-studio-emr-on-eks/tmp/delta/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdd88248-4e4d-4e83-b297-42e9976aae2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:00:46.415158Z",
     "iopub.status.busy": "2022-12-06T17:00:46.414410Z",
     "iopub.status.idle": "2022-12-06T17:00:46.572531Z",
     "shell.execute_reply": "2022-12-06T17:00:46.570995Z",
     "shell.execute_reply.started": "2022-12-06T17:00:46.415106Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(deltaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547daf28-5ddd-46f9-a96d-f1b8e5ec5c07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:00:48.439033Z",
     "iopub.status.busy": "2022-12-06T17:00:48.438253Z",
     "iopub.status.idle": "2022-12-06T17:01:03.539356Z",
     "shell.execute_reply": "2022-12-06T17:01:03.537998Z",
     "shell.execute_reply.started": "2022-12-06T17:00:48.438980Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-12-06 16:59:54|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-06 16:59:54|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-06 16:59:54|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-06 16:59:54|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-06 16:59:54|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-06 16:59:54|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7088817e-7720-440f-a95f-1c8c15387e0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:01:43.441661Z",
     "iopub.status.busy": "2022-12-06T17:01:43.440879Z",
     "iopub.status.idle": "2022-12-06T17:01:43.490985Z",
     "shell.execute_reply": "2022-12-06T17:01:43.489677Z",
     "shell.execute_reply.started": "2022-12-06T17:01:43.441596Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Update couple fo existing records. Update cust_age to 35 for cust_id 1 and cust_loc from MD to CA for cust_id 4\n",
    "#Insert a new Record cust_id = 7\n",
    "UpdateData = [\n",
    "    (1,'Prasad Nadig', 35, 'NJ','2022-01-01', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (4,'Solana', 55, 'MD', '2022-01-04', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "    (7,'Algorand', 55, 'NC', '2022-01-07', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "updateDF = spark.createDataFrame(data=UpdateData,schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9621f211-bd48-4690-bf5b-1b9567e6b063",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.alias('targetData') \\\n",
    "    .merge(\n",
    "        updateDF.alias('updatedData'),\n",
    "        'targetData.cust_id = updatedData.cust_id') \\\n",
    "    .whenMatchedUpdate( set = \n",
    "        {\n",
    "            \"cust_id\": \"updatedData.cust_id\",\n",
    "            \"cust_name\": \"updatedData.cust_name\",\n",
    "            \"cust_age\": \"updatedData.cust_age\",\n",
    "            \"cust_loc\": \"updatedData.cust_loc\",\n",
    "            \"create_date\": \"updatedData.create_date\",\n",
    "            \"last_updated_time\": \"updatedData.last_updated_time\"\n",
    "        } \n",
    "                      ) \\\n",
    "    .whenNotMatchedInsert(values = \n",
    "        {\n",
    "            \"cust_id\": \"updatedData.cust_id\",\n",
    "            \"cust_name\": \"updatedData.cust_name\",\n",
    "            \"cust_age\": \"updatedData.cust_age\",\n",
    "            \"cust_loc\": \"updatedData.cust_loc\",\n",
    "            \"create_date\": \"updatedData.create_date\",\n",
    "            \"last_updated_time\": \"updatedData.last_updated_time\"\n",
    "        } \n",
    "                         ) \\\n",
    "    .execute()\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa7c648-4799-4b15-9b91-bb95decdba09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:01:49.756323Z",
     "iopub.status.busy": "2022-12-06T17:01:49.755558Z",
     "iopub.status.idle": "2022-12-06T17:02:07.630075Z",
     "shell.execute_reply": "2022-12-06T17:02:07.622779Z",
     "shell.execute_reply.started": "2022-12-06T17:01:49.756271Z"
    }
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.alias('targetData') \\\n",
    "    .merge(\n",
    "        updateDF.alias('updatedData'),\n",
    "        'targetData.cust_id = updatedData.cust_id') \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffddb15d-4516-48fd-90c8-db882d2a44d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:02:12.341592Z",
     "iopub.status.busy": "2022-12-06T17:02:12.340837Z",
     "iopub.status.idle": "2022-12-06T17:02:20.054331Z",
     "shell.execute_reply": "2022-12-06T17:02:20.053006Z",
     "shell.execute_reply.started": "2022-12-06T17:02:12.341537Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      35|      NJ| 2022-01-01|2022-12-06 17:01:43|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-06 16:59:54|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-06 16:59:54|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-06 17:01:43|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-06 16:59:54|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-06 16:59:54|\n",
      "|      7|    Algorand|      55|      NC| 2022-01-07|2022-12-06 17:01:43|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta_updates = spark.read.format(\"delta\").load(deltaPath)\n",
    "df_delta_updates.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d39915f4-be04-4092-86bd-1a0ed2a0c1a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:02:23.248524Z",
     "iopub.status.busy": "2022-12-06T17:02:23.247777Z",
     "iopub.status.idle": "2022-12-06T17:02:23.292265Z",
     "shell.execute_reply": "2022-12-06T17:02:23.290740Z",
     "shell.execute_reply.started": "2022-12-06T17:02:23.248468Z"
    }
   },
   "outputs": [],
   "source": [
    "#Update couple fo existing records. Update cust_age to 35 for cust_id 1 and cust_loc from MD to CA for cust_id 4\n",
    "#Insert a new Record cust_id = 7\n",
    "DeleteData = [\n",
    "    (6,'Link', 45, 'NJ', '2022-01-06', datetime.strptime(datetime.now().strftime(\"%Y-%d-%m %H:%M:%S\"), \"%Y-%d-%m %H:%M:%S\")),\n",
    "]\n",
    "\n",
    "#Define schema for the source data\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cust_id\",IntegerType(),True), \\\n",
    "    StructField(\"cust_name\",StringType(),True), \\\n",
    "    StructField(\"cust_age\",IntegerType(),True), \\\n",
    "    StructField(\"cust_loc\",StringType(),True), \\\n",
    "    StructField(\"create_date\", StringType(), True), \\\n",
    "    StructField(\"last_updated_time\", TimestampType(), True)\n",
    "  ])\n",
    "\n",
    "#Create dataframe from the input data and the corresponding schema\n",
    "deleteDF = spark.createDataFrame(data=DeleteData,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe1b12d-9b08-47c7-8832-ce4dced716c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:02:27.882489Z",
     "iopub.status.busy": "2022-12-06T17:02:27.881687Z",
     "iopub.status.idle": "2022-12-06T17:02:37.911607Z",
     "shell.execute_reply": "2022-12-06T17:02:37.909922Z",
     "shell.execute_reply.started": "2022-12-06T17:02:27.882436Z"
    }
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "deltaTable.alias('targetData') \\\n",
    "    .merge(\n",
    "        deleteDF.alias('deletedData'),\n",
    "        'targetData.cust_id = deletedData.cust_id') \\\n",
    "    .whenMatchedDelete()\\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5d3b17-0424-4702-ac86-351dc1d78eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:02:40.887100Z",
     "iopub.status.busy": "2022-12-06T17:02:40.886364Z",
     "iopub.status.idle": "2022-12-06T17:02:42.197728Z",
     "shell.execute_reply": "2022-12-06T17:02:42.196352Z",
     "shell.execute_reply.started": "2022-12-06T17:02:40.886911Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      35|      NJ| 2022-01-01|2022-12-06 17:01:43|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-06 16:59:54|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-06 16:59:54|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-06 17:01:43|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-06 16:59:54|\n",
      "|      7|    Algorand|      55|      NC| 2022-01-07|2022-12-06 17:01:43|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_delta_deletes = spark.read.format(\"delta\").load(deltaPath)\n",
    "df_delta_deletes.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2877bccb-a9dc-4677-bfee-954a3700b17a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:02:45.005610Z",
     "iopub.status.busy": "2022-12-06T17:02:45.004851Z",
     "iopub.status.idle": "2022-12-06T17:02:47.037280Z",
     "shell.execute_reply": "2022-12-06T17:02:47.036036Z",
     "shell.execute_reply.started": "2022-12-06T17:02:45.005556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      2|2022-12-06 17:02:34|  null|    null|    MERGE|{predicate -> (ta...|null|    null|     null|          1|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|      1|2022-12-06 17:02:02|  null|    null|    MERGE|{predicate -> (ta...|null|    null|     null|          0|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|      0|2022-12-06 17:00:27|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 6, n...|        null|Apache-Spark/3.3....|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a7d4b03-45bc-4ad6-8422-7d701fa91753",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:02:54.571386Z",
     "iopub.status.busy": "2022-12-06T17:02:54.570622Z",
     "iopub.status.idle": "2022-12-06T17:02:58.256851Z",
     "shell.execute_reply": "2022-12-06T17:02:58.255923Z",
     "shell.execute_reply.started": "2022-12-06T17:02:54.571331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      25|      NJ| 2022-01-01|2022-12-06 16:59:54|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-06 16:59:54|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-06 16:59:54|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-06 16:59:54|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-06 16:59:54|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-06 16:59:54|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beforeUpsert = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(deltaPath)\n",
    "beforeUpsert.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "299eee86-2fd3-4c87-bb0a-7875b6b5ae00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-06T17:03:31.301287Z",
     "iopub.status.busy": "2022-12-06T17:03:31.300525Z",
     "iopub.status.idle": "2022-12-06T17:03:35.045785Z",
     "shell.execute_reply": "2022-12-06T17:03:35.044423Z",
     "shell.execute_reply.started": "2022-12-06T17:03:31.301234Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|cust_id|   cust_name|cust_age|cust_loc|create_date|  last_updated_time|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "|      1|Prasad Nadig|      35|      NJ| 2022-01-01|2022-12-06 17:01:43|\n",
      "|      2|    Ethereum|      80|      NY| 2022-01-02|2022-12-06 16:59:54|\n",
      "|      3|      Cosmos|      25|      PA| 2022-01-03|2022-12-06 16:59:54|\n",
      "|      4|      Solana|      55|      MD| 2022-01-04|2022-12-06 17:01:43|\n",
      "|      5|     Carnado|      15|      TX| 2022-01-05|2022-12-06 16:59:54|\n",
      "|      6|        Link|      45|      NJ| 2022-01-06|2022-12-06 16:59:54|\n",
      "|      7|    Algorand|      55|      NC| 2022-01-07|2022-12-06 17:01:43|\n",
      "+-------+------------+--------+--------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beforeDelete = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(deltaPath)\n",
    "beforeDelete.sort('cust_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833da195-81bf-4e81-9a0b-d36811b58e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Kubernetes)",
   "language": "python",
   "name": "spark_python_kubernetes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
